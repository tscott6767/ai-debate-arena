ğŸ§ â€¯AIâ€¯Codingâ€¯Arenaâ€¯â€”â€¯Androidâ€¯SAFâ€¯Edition
A brutal, adversarial, selfâ€‘improving multiâ€‘agent coding platform.
Twoâ€¯LLMsâ€¯enter. Oneâ€¯survives.
They fight to produce the best Androidâ€¯14 Fileâ€¯Explorer using only the Storageâ€¯Accessâ€¯Frameworkâ€¯(SAF).
A mercilessâ€¯Judge ensures modernity, killing any code that touches legacy storage or fails to compile.
Only perfect, selfâ€‘evolving, standardsâ€‘compliant code climbs the leaderboard.

ğŸ§©â€¯Coreâ€¯Concept
AIâ€¯Codingâ€¯Arena is a multiâ€‘agent competitive softwareâ€‘generation platform.
Itâ€™s designed to test, evolve, and refine LLMâ€‘generated code using domainâ€‘specific judges and continuous feedback loops.
Each Round:
1.â€¯Twoâ€¯Agentsâ€¯(e.g.,â€¯Qwenâ€¯30Bâ€¯vsâ€¯CodeLlamaâ€¯13B)â€¯receiveâ€¯theâ€¯sameâ€¯challenge.
2.â€¯Eachâ€¯outputsâ€¯pureâ€¯codeâ€¯(HTML,â€¯Java,â€¯Rust,â€¯etc.),â€¯followingâ€¯formattingâ€¯rulesâ€¯enforcedâ€¯byâ€¯prompts.py.
3.â€¯Theâ€¯Judgeâ€¯(judge.py)â€¯immediatelyâ€¯scoresâ€¯eachâ€¯submission.
4.â€¯Controllerâ€¯(controller.py)â€¯logsâ€¯verdictsâ€¯andâ€¯automaticallyâ€¯feedsâ€¯theâ€¯judge'sâ€¯feedbackâ€¯intoâ€¯freshâ€¯promptsâ€¯forâ€¯theâ€¯nextâ€¯round.
5.â€¯Modelsâ€¯iterate,â€¯improve,â€¯andâ€¯competeâ€¯untilâ€¯onlyâ€¯stable,â€¯compilingâ€¯codeâ€¯remains.
Overâ€¯timeâ€¯theâ€¯Arenaâ€¯becomesâ€¯aâ€¯selfâ€‘containedâ€¯autonomousâ€¯codingâ€¯ecosystem:
LLMsâ€¯â†’â€¯produceâ€¯â†’â€¯testedâ€¯â†’â€¯scoredâ€¯â†’â€¯reâ€‘promptedâ€¯â†’â€¯refined.

âš”ï¸â€¯Rulesâ€¯ofâ€¯Combatâ€¯(Androidâ€¯Edition)
Theâ€¯Judgeâ€¯executesâ€¯immediatelyâ€¯for:

â˜ ï¸â€¯Useâ€¯ofâ€¯java.io.File,â€¯getExternalStorageDirectory(),â€¯getParentFile(),â€¯Uri.fromFile(),â€¯orâ€¯anyâ€¯deprecatedâ€¯Fileâ€¯API.
ğŸ’¬â€¯Nonâ€‘codeâ€¯outputâ€¯(bulletâ€¯lists,â€¯releaseâ€¯notes,â€¯apologies).
ğŸ’¤â€¯Regression,â€¯hallucination,â€¯orâ€¯reintroductionâ€¯ofâ€¯legacyâ€¯logic.

Victoryâ€¯Conditions:

Fullâ€¯SAFâ€¯complianceâ€¯(ACTION_OPEN_DOCUMENT_TREE,â€¯takePersistableUriPermission,â€¯DocumentFile.fromTreeUri,â€¯DocumentsContract).
Modernâ€¯Androidâ€¯14â€¯SDKâ€¯compatibleâ€¯code.
Compilesâ€¯cleanlyâ€¯andâ€¯performsâ€¯allâ€¯CRUDâ€¯operationsâ€¯underâ€¯scopedâ€¯storage.


ğŸ—ï¸â€¯Projectâ€¯Structure



File
Purpose




prompts.py
Enforcesâ€¯â€œcodeâ€‘onlyâ€â€¯LLMâ€¯responses;â€¯injectsâ€¯pastâ€¯judgeâ€¯feedbackâ€¯intoâ€¯nextâ€¯promptâ€¯forâ€¯selfâ€‘improvement.


judge.py
Domainâ€‘specificâ€¯ruleâ€¯engine.â€¯Detectsâ€¯bannedâ€¯patterns,â€¯validatesâ€¯SAFâ€¯usage,â€¯andâ€¯scoresâ€¯submissionsâ€¯(0â€“10).


controller.py
Theâ€¯Arenaâ€¯engine.â€¯Orchestratesâ€¯rounds,â€¯launchesâ€¯models,â€¯logsâ€¯resultsâ€¯toâ€¯debates.db,â€¯andâ€¯reâ€‘feedsâ€¯verdictâ€¯backâ€¯toâ€¯agents.


main.pyâ€¯/â€¯API
FastAPIâ€¯endpointâ€¯forâ€¯submissionsâ€¯andâ€¯liveâ€¯resultsâ€¯streaming.


README.md
Thisâ€¯fileâ€¯â€”â€¯manifestoâ€¯+â€¯roadmap.




ğŸ”„â€¯Autonomousâ€¯Refâ€‘Feedâ€¯System
Coreâ€¯Mechanism:
1.â€¯Theâ€¯Judgeâ€¯returnsâ€¯verdictsâ€¯asâ€¯structuredâ€¯textâ€¯(score,â€¯reasoning,â€¯banned_hits,â€¯missing_patterns).
2.â€¯controller.pyâ€¯parsesâ€¯thisâ€¯verdictâ€¯intoâ€¯aâ€¯conciseâ€¯summary.
3.â€¯Thatâ€¯summaryâ€¯isâ€¯reâ€‘appendedâ€¯toâ€¯eachâ€¯modelâ€™sâ€¯promptâ€¯forâ€¯theâ€¯nextâ€¯round,â€¯e.g.:
Previousâ€¯Verdictâ€¯Summary:
â€¢â€¯Failedâ€¯SAFâ€¯validationâ€¯â€”â€¯missingâ€¯takePersistableUriPermission
â€¢â€¯Usedâ€¯grantUriPermissionâ€¯instead
â€¢â€¯Requiredâ€¯patterns:â€¯ACTION_OPEN_DOCUMENT_TREE,â€¯takePersistableUriPermission,â€¯DocumentFile.fromTreeUri

â†’â€¯Fixâ€¯allâ€¯issuesâ€¯andâ€¯resubmitâ€¯entireâ€¯MainActivity.javaâ€¯asâ€¯Androidâ€¯14â€¯codeâ€‘only.

Eachâ€¯roundâ€¯thereforeâ€¯includesâ€¯semanticallyâ€¯relevantâ€¯feedbackâ€¯fromâ€¯theâ€¯Judge,â€¯drivingâ€¯modelâ€¯optimizationâ€¯withoutâ€¯humanâ€¯input.

âš–ï¸â€¯Judgeâ€¯Evolutionâ€¯andâ€¯Metaâ€‘Learningâ€¯(ğŸ§ â€¯New)
Theâ€¯Judgeâ€¯isâ€¯noâ€¯longerâ€¯staticâ€¯â€”â€¯itâ€¯evolvesâ€¯too.
Yourâ€¯Arenaâ€™sâ€¯nextâ€¯frontierâ€¯isâ€¯coâ€‘evolution:â€¯notâ€¯justâ€¯modelsâ€¯learningâ€¯toâ€¯codeâ€¯better,â€¯butâ€¯judgesâ€¯learningâ€¯toâ€¯judgeâ€¯better.
Currentâ€¯Stage:â€¯Regexâ€¯patternâ€¯matchingâ€¯andâ€¯ruleâ€‘basedâ€¯scoring.
Nextâ€¯Stages:
1.â€¯Semanticâ€¯(Aâ€¯Sâ€¯Tâ€¯Parsing)â€¯â€“â€¯Integrateâ€¯JavaParserâ€¯orâ€¯KSPâ€¯forâ€¯trueâ€¯syntacticâ€¯validation.
2.â€¯Compilationâ€¯Gradingâ€¯â€“â€¯Dockerizedâ€¯Gradleâ€¯buildsâ€¯confirmâ€¯codeâ€¯compilesâ€¯andâ€¯producesâ€¯workingâ€¯APKs.
3.â€¯Runtimeâ€¯Simulationâ€¯â€“â€¯Sandboxâ€¯eachâ€¯APKâ€¯inâ€¯anâ€¯emulatorâ€¯(AVDâ€¯/â€¯Robolectric)â€¯toâ€¯executeâ€¯fileâ€¯operationsâ€¯andâ€¯captureâ€¯logsâ€¯forâ€¯scoring.
4.â€¯Weightedâ€¯Scoringâ€¯â€“â€¯Combineâ€¯staticâ€¯checksâ€¯(40â€¯%),â€¯compileâ€¯statusâ€¯(40â€¯%),â€¯andâ€¯behaviorâ€¯testsâ€¯(20â€¯%).
5.â€¯Metaâ€‘Feedbackâ€¯Loopâ€¯â€“â€¯Storeâ€¯casesâ€¯whereâ€¯Judgeâ€¯verdictsâ€¯misalignâ€¯withâ€¯realâ€¯runtimeâ€¯testsâ€¯â†’â€¯automaticallyâ€¯updateâ€¯scoringâ€¯rulesâ€¯=â€¯selfâ€‘correctingâ€¯Judge.
In futureâ€¯phases,â€¯theâ€¯Judgeâ€¯itselfâ€¯becomesâ€¯anâ€¯LLMâ€¯QAâ€¯agentâ€¯trainedâ€¯onâ€¯itsâ€¯ownâ€¯mistakes.
The arenaâ€¯thenâ€¯hostsâ€¯twoâ€¯speciesâ€¯ofâ€¯AIâ€¯coâ€‘evolving:â€¯codersâ€¯andâ€¯critics.

ğŸš€â€¯Runningâ€¯theâ€¯Arena
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000

Accessâ€¯theâ€¯Arenaâ€¯at:â€¯http://localhost:8000
###â€¯APIâ€¯Endpoints
-â€¯POSTâ€¯/submitâ€¯â€“â€¯submitâ€¯aâ€¯pairâ€¯ofâ€¯codesâ€¯(Aâ€¯andâ€¯B).
-â€¯GETâ€¯/verdictsâ€¯â€“â€¯streamâ€¯latestâ€¯results.
-â€¯GETâ€¯/leaderboardâ€¯â€“â€¯ELOâ€‘styleâ€¯rankingâ€¯ofâ€¯modelsâ€¯byâ€¯winâ€¯rateâ€¯andâ€¯averageâ€¯score.
ğŸ§¾â€¯README.md Additions
Add this new section below your setup instructions so anyone cloning later can run it without confusion:

ğŸ’¬â€¯Web Interfaceâ€¯â€“â€¯Updated Tokenâ€¯System
Recent versions ofâ€¯AIâ€¯Debateâ€¯Arena use aâ€¯tokenâ€‘based topic transfer mechanism to handle large debate histories safely:


Continuations:
Each new debate automatically fetches the previous transcript via
GETâ€¯/api/continuation?limit=1&round_no=<n>.


Token registration:
The browser thenâ€¯posts the long prompt body to
POSTâ€¯/api/register_topicâ€¯â†’â€¯receives a shortâ€¯token, e.g.â€¯02353ffc732f56f5.


WebSocket startup:
The debate begins with
ws://<host>/ws/debate?token=<token>&rounds=...,
eliminating URLâ€¯length limitations and preventingâ€¯400â€¯errors even withâ€¯veryâ€¯longâ€¯histories.


Server preload:
Topics are stored temporarily inâ€¯memoryâ€¯(TOPIC_CACHE)â€¯and removed after retrieval to keep memory use low.


Typical workflow
python3 -m uvicorn main:app --reload --host 0.0.0.0 --port 8000

Then open http://localhost:8000/static/index.html
â†’â€¯press STARTâ€¯DEBATEâ€¯to begin.


Expected log output
TOPIC length: 79004
Roundâ€¯13â€¯â€”â€¯Continuation ofâ€¯Roundâ€¯12
...

This confirms the previous transcript was loaded successfully.
ğŸ—ºï¸â€¯Fullâ€¯Roadmap
###â€¯Phaseâ€¯Iâ€¯â€”â€¯Foundationâ€¯(âœ…â€¯Complete)
âœ…â€¯Androidâ€¯14â€¯Judgeâ€¯eliminatesâ€¯legacyâ€¯Fileâ€¯APIs.
âœ…â€¯SAFâ€¯structuralâ€¯validationâ€¯withâ€¯regexâ€¯andâ€¯scoring.
âœ…â€¯Arenaâ€¯Controllerâ€¯handlesâ€¯A/Bâ€¯generationâ€¯cyclesâ€¯andâ€¯storesâ€¯debatesâ€¯inâ€¯SQLite.
###â€¯Phaseâ€¯IIâ€¯â€”â€¯Autonomousâ€¯Feedbackâ€¯Loopâ€¯(ğŸš€â€¯Active)
[x]â€¯Verdictâ€¯parserâ€¯extractsâ€¯structuredâ€¯feedback.
[x]â€¯Reâ€‘feedâ€¯systemâ€¯autoâ€‘insertsâ€¯Judgeâ€¯assessmentâ€¯intoâ€¯nextâ€¯modelâ€¯prompt.
[x]â€¯Runâ€¯continuousâ€¯evolutionâ€¯roundsâ€¯untilâ€¯stability.
###â€¯Phaseâ€¯IIIâ€¯â€”â€¯Beyondâ€¯Regexâ€¯(ğŸ§ â€¯Planned)
[ ]â€¯Integrateâ€¯ASTâ€‘basedâ€¯analyzersâ€¯(JavaParserâ€¯/â€¯Kotlinâ€¯KSP).
[ ]â€¯Dockerizedâ€¯Gradleâ€¯buildâ€¯toâ€¯confirmâ€¯actualâ€¯compilationâ€¯&â€¯APKâ€¯build.
[ ]â€¯Runtimeâ€¯sandboxâ€¯toâ€¯executeâ€¯instrumentedâ€¯testsâ€¯onâ€¯emulatorâ€¯containersâ€¯(Googleâ€¯AVDâ€¯+â€¯CIâ€¯pipeline).
[ ]â€¯Addâ€¯Judgeâ€¯Metaâ€‘Learningâ€¯Pipelineâ€¯forâ€¯selfâ€‘trainingâ€¯onâ€¯verdictâ€¯mismatches.
###â€¯Phaseâ€¯IVâ€¯â€”â€¯Crossâ€‘Languageâ€¯Arenasâ€¯(ğŸ”¥â€¯Next)
[ ]â€¯Rustâ€¯Editionâ€¯â€“â€¯Judgeâ€¯rewardsâ€¯async/await,â€¯unitâ€¯tests,â€¯forbidsâ€¯println!()â€¯debugging.
[ ]â€¯Pythonâ€¯Editionâ€¯â€“â€¯forbidsâ€¯print()â€¯logs,â€¯rewardsâ€¯asyncioâ€¯andâ€¯pytestâ€¯coverage.
[ ]â€¯C++â€¯Editionâ€¯â€“â€¯penalizesâ€¯manualâ€¯new/delete,â€¯rewardsâ€¯RAIIâ€¯andâ€¯constexpr.
[ ]â€¯Webâ€¯Editionâ€¯â€“â€¯enforcesâ€¯fetchâ€¯withâ€¯async/awaitâ€¯andâ€¯PWAâ€¯serviceâ€¯workers.
###â€¯Phaseâ€¯Vâ€¯â€”â€¯Autonomousâ€¯Scalingâ€¯(âš™ï¸â€¯Future)
[ ]â€¯Multiâ€‘agentâ€¯schedulerâ€¯forâ€¯50+â€¯simultaneousâ€¯battles.
[ ]â€¯Distributedâ€¯storageâ€¯andâ€¯ELOâ€¯scoreboards.
[ ]â€¯Automaticâ€¯modelâ€¯deploymentâ€¯andâ€¯rollbackâ€¯basedâ€¯onâ€¯winâ€¯rateâ€¯thresholds.
[ ]â€¯Evolvingâ€¯Judgesâ€¯thatâ€¯retrainâ€¯criteriaâ€¯fromâ€¯humanâ€¯reviewâ€¯orâ€¯runtimeâ€¯telemetry.
###â€¯Phaseâ€¯VIâ€¯â€”â€¯Productionâ€¯/â€¯Researchâ€¯Launchâ€¯(ğŸŒâ€¯Future)
[ ]â€¯Publicâ€¯leaderboardâ€¯portal.
[ ]â€¯Publishâ€¯whiteâ€¯paper:â€¯â€œAdversarialâ€¯Coâ€‘evolutionâ€¯ofâ€¯Codeâ€¯andâ€¯Evaluationâ€¯inâ€¯Multiâ€‘LLMâ€¯Systems.â€
[ ]â€¯Integrationâ€¯withâ€¯GitHubâ€¯Actionsâ€¯â†’â€¯AIâ€¯botsâ€¯thatâ€¯autonomouslyâ€¯openâ€¯PRsâ€¯ofâ€¯winningâ€¯code.

ğŸ”¬â€¯Extendingâ€¯forâ€¯Otherâ€¯Domains
Toâ€¯createâ€¯newâ€¯Arenas:
1.â€¯Duplicateâ€¯thisâ€¯repo.
2.â€¯Replaceâ€¯language/frameworkâ€¯identifiersâ€¯inâ€¯prompts.py.
3.â€¯Designâ€¯aâ€¯newâ€¯judge.pyâ€¯withâ€¯yourâ€¯domainâ€™sâ€¯rules.
4.â€¯Optionallyâ€¯addâ€¯compiler/testâ€¯runnerâ€¯forâ€¯realâ€¯executionâ€¯scoring.
Exampleâ€¯â€“â€¯Rustâ€¯Edition:
BANNED = [r"println!", r"thread::sleep", r"unwrap$$"]
REWARD = [r"async fn", r"tokio::main", r"#[test]"]


ğŸâ€¯Vision

â€œAIâ€¯Codingâ€¯Arenaâ€¯isâ€¯theâ€¯evolutionaryâ€¯pressureâ€¯codeâ€¯hasâ€¯neverâ€¯had.â€

Eachâ€¯generationâ€¯learnsâ€¯fromâ€¯theâ€¯bloodâ€¯ofâ€¯itsâ€¯predecessor.
Judgesâ€¯enforceâ€¯truth;â€¯modelsâ€¯fightâ€¯forâ€¯perfection.
Theâ€¯endâ€¯gameâ€¯isâ€¯anâ€¯autonomousâ€¯softwareâ€¯ecosystemâ€¯whereâ€¯AIâ€¯agentsâ€¯continuouslyâ€¯generate,â€¯test,â€¯andâ€¯refineâ€¯theâ€¯worldâ€™sâ€¯codeâ€¯basesâ€¯â€”â€¯noâ€¯humanâ€¯babysittingâ€¯required.

###â€¯Runâ€¯yourâ€¯ownâ€¯Arena.â€¯Spawnâ€¯newâ€¯Judges.â€¯Letâ€¯theâ€¯LLMsâ€¯fight.
python controller.py

AIâ€¯Codingâ€¯Arenaâ€¯â€”â€¯Theâ€¯futureâ€¯ofâ€¯autonomousâ€¯softwareâ€¯evolution
