ğŸ§  AIâ€¯Codingâ€¯Arena â€” Androidâ€¯SAFâ€¯Edition
A brutal, adversarial, selfâ€‘improving multiâ€‘agent coding platform.
Twoâ€¯LLMsâ€¯enter. Oneâ€¯survives.
They fight to produce the best Androidâ€¯14 Fileâ€¯Explorer using only the Storageâ€¯Accessâ€¯Frameworkâ€¯(SAF).
A mercilessâ€¯Judge ensures modernity, killing any code that touches legacy storage or fails to compile.
Only perfect, selfâ€‘evolving, standardsâ€‘compliant code climbs the leaderboard.

ğŸ§©â€¯Coreâ€¯Concept
AIâ€¯Codingâ€¯Arena is a multiâ€‘agent competitive softwareâ€‘generation platform.
Itâ€™s designed to test, evolve, and refine LLMâ€‘generated code using domainâ€‘specific judges and continuous feedback loops.
Each Round:
1.â€¯Twoâ€¯Agents (e.g., Qwenâ€¯30Bâ€¯vsâ€¯CodeLlamaâ€¯13B) receive the same challenge.
2.â€¯Eachâ€¯outputsâ€¯pureâ€¯codeâ€¯(HTML,â€¯Java,â€¯Rust,â€¯etc.), following formatting rules enforced byâ€¯prompts.py.
3.â€¯Theâ€¯Judge (judge.py) immediately scores eachâ€¯submission.
4.â€¯Controller (controller.py) logsâ€¯verdictsâ€¯and automatically feeds theâ€¯judge'sâ€¯feedbackâ€¯intoâ€¯freshâ€¯promptsâ€¯forâ€¯the nextâ€¯round.
5.â€¯Modelsâ€¯iterate,â€¯improve,â€¯andâ€¯competeâ€¯untilâ€¯only stable,â€¯compilingâ€¯codeâ€¯remains.
Overâ€¯timeâ€¯theâ€¯Arena becomes a selfâ€‘contained autonomous coding ecosystem:
LLMsâ€¯â†’â€¯produceâ€¯â†’â€¯testedâ€¯â†’â€¯scoredâ€¯â†’â€¯reâ€‘promptedâ€¯â†’â€¯refined.

âš”ï¸â€¯Rulesâ€¯ofâ€¯Combat (Androidâ€¯Edition)
Theâ€¯Judgeâ€¯executesâ€¯immediatelyâ€¯for:

â˜ ï¸â€¯Useâ€¯ofâ€¯java.io.File,â€¯getExternalStorageDirectory(),â€¯getParentFile(),â€¯Uri.fromFile(),â€¯orâ€¯anyâ€¯deprecatedâ€¯Fileâ€¯API.
ğŸ’¬â€¯Nonâ€‘codeâ€¯outputâ€¯(bulletâ€¯lists,â€¯releaseâ€¯notes,â€¯apologies).
ğŸ’¤â€¯Regression,â€¯hallucination,â€¯orâ€¯reintroductionâ€¯ofâ€¯legacyâ€¯logic.

Victory Conditions:

Fullâ€¯SAFâ€¯compliance (ACTION_OPEN_DOCUMENT_TREE,â€¯takePersistableUriPermission,â€¯DocumentFile.fromTreeUri,â€¯DocumentsContract).
Modernâ€¯Androidâ€¯14â€¯SDKâ€¯compatibleâ€¯code.
Compilesâ€¯cleanlyâ€¯andâ€¯performsâ€¯allâ€¯CRUDâ€¯operationsâ€¯underâ€¯scopedâ€¯storage.


ğŸ—ï¸â€¯Projectâ€¯Structure



File
Purpose




prompts.py
Enforces â€œcodeâ€‘onlyâ€ LLMâ€¯responses; injects pastâ€¯judgeâ€¯feedback into nextâ€¯prompt for selfâ€‘improvement.


judge.py
Domainâ€‘specific ruleâ€¯engine. Detectsâ€¯bannedâ€¯patterns,â€¯validatesâ€¯SAFâ€¯usage,â€¯andâ€¯scoresâ€¯submissionsâ€¯(0â€“10).


controller.py
Theâ€¯Arenaâ€¯engine. Orchestratesâ€¯rounds,â€¯launchesâ€¯models, logsâ€¯resultsâ€¯toâ€¯debates.db,â€¯andâ€¯reâ€‘feedsâ€¯verdictâ€¯backâ€¯toâ€¯agents.


main.pyâ€¯/â€¯API
FastAPIâ€¯endpointâ€¯forâ€¯submissionsâ€¯andâ€¯liveâ€¯resultsâ€¯streaming.


README.md
Thisâ€¯fileâ€¯â€”â€¯manifestoâ€¯+â€¯roadmap.




ğŸ”„â€¯Autonomousâ€¯Refâ€‘Feedâ€¯System
Coreâ€¯Mechanism:

Theâ€¯Judgeâ€¯returnsâ€¯verdictsâ€¯asâ€¯structuredâ€¯textâ€¯(score,â€¯reasoning,â€¯banned_hits,â€¯missing_patterns).
controller.pyâ€¯parsesâ€¯thisâ€¯verdictâ€¯intoâ€¯aâ€¯conciseâ€¯summary.
Thatâ€¯summaryâ€¯isâ€¯reâ€‘appendedâ€¯toâ€¯eachâ€¯modelâ€™sâ€¯promptâ€¯forâ€¯theâ€¯nextâ€¯round,â€¯e.g.:

Previousâ€¯Verdictâ€¯Summary:
â€¢â€¯Failedâ€¯SAFâ€¯validationâ€¯â€”â€¯missingâ€¯takePersistableUriPermission
â€¢â€¯Usedâ€¯grantUriPermissionâ€¯instead
â€¢â€¯Requiredâ€¯patterns:â€¯ACTION_OPEN_DOCUMENT_TREE,â€¯takePersistableUriPermission,â€¯DocumentFile.fromTreeUri

â†’â€¯Fixâ€¯allâ€¯issuesâ€¯andâ€¯resubmitâ€¯entireâ€¯MainActivity.javaâ€¯asâ€¯Androidâ€¯14â€¯codeâ€‘only.

Eachâ€¯roundâ€¯thereforeâ€¯includesâ€¯semanticallyâ€¯relevantâ€¯feedbackâ€¯fromâ€¯theâ€¯Judge,â€¯drivingâ€¯modelâ€¯optimizationâ€¯withoutâ€¯humanâ€¯input.

ğŸš€â€¯Runningâ€¯theâ€¯Arena
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000

Accessâ€¯theâ€¯Arenaâ€¯at:â€¯http://localhost:8000
###â€¯APIâ€¯Endpoints

POSTâ€¯/submitâ€¯â€“â€¯submitâ€¯aâ€¯pairâ€¯ofâ€¯codesâ€¯(Aâ€¯andâ€¯B).
GETâ€¯/verdictsâ€¯â€“â€¯streamâ€¯latestâ€¯results.
GETâ€¯/leaderboardâ€¯â€“â€¯ELOâ€‘styleâ€¯rankingâ€¯ofâ€¯modelsâ€¯byâ€¯winâ€¯rateâ€¯andâ€¯averageâ€¯score.


ğŸ—ºï¸â€¯Fullâ€¯Roadmap
###â€¯Phaseâ€¯Iâ€¯â€”â€¯Foundationâ€¯(âœ…â€¯Complete)

âœ…â€¯Androidâ€¯14â€¯judgeâ€¯eliminatesâ€¯legacyâ€¯Fileâ€¯APIs.
âœ…â€¯SAFâ€¯structuralâ€¯validationâ€¯withâ€¯regexâ€¯andâ€¯scoring.
âœ…â€¯Arenaâ€¯Controllerâ€¯handlesâ€¯A/Bâ€¯generationâ€¯cyclesâ€¯andâ€¯storesâ€¯debatesâ€¯inâ€¯SQLite.

###â€¯Phaseâ€¯IIâ€¯â€”â€¯Autonomousâ€¯Feedbackâ€¯Loopâ€¯(ğŸš€â€¯Active)

[x]â€¯Verdictâ€¯parserâ€¯extractsâ€¯structuredâ€¯feedback.
[x]â€¯Reâ€‘feedâ€¯systemâ€¯autoâ€‘insertsâ€¯judgeâ€¯assessmentâ€¯intoâ€¯nextâ€¯modelâ€¯prompt.
[x]â€¯Runâ€¯continuousâ€¯evolutionâ€¯roundsâ€¯untilâ€¯stability.

###â€¯Phaseâ€¯IIIâ€¯â€”â€¯Beyondâ€¯Regexâ€¯(ğŸ§ â€¯Planned)

[ ]â€¯Integrateâ€¯ASTâ€‘basedâ€¯analyzersâ€¯(JavaParserâ€¯/â€¯Kotlinâ€¯KSP).
[ ]â€¯Dockerizedâ€¯Gradleâ€¯buildâ€¯toâ€¯confirmâ€¯actualâ€¯compilationâ€¯&â€¯APKâ€¯build.
[ ]â€¯Runtimeâ€¯sandboxâ€¯toâ€¯executeâ€¯instrumentedâ€¯testsâ€¯onâ€¯emulatorâ€¯containersâ€¯(Googleâ€¯AVDâ€¯+â€¯CIâ€¯pipeline).

###â€¯Phaseâ€¯IVâ€¯â€”â€¯Crossâ€‘Languageâ€¯Arenasâ€¯(ğŸ”¥â€¯Next)

[ ]â€¯Rustâ€¯Editionâ€¯â€“â€¯Judgeâ€¯rewardsâ€¯async/await,â€¯unitâ€¯tests,â€¯forbidsâ€¯println!()â€¯debugging.
[ ]â€¯Pythonâ€¯Editionâ€¯â€“â€¯forbidsâ€¯print()â€¯logs,â€¯rewardsâ€¯asyncioâ€¯andâ€¯pytestâ€¯coverage.
[ ]â€¯C++â€¯Editionâ€¯â€“â€¯penalizesâ€¯manualâ€¯new/delete,â€¯rewardsâ€¯RAIIâ€¯andâ€¯constexprâ€¯usage.
[ ]â€¯Webâ€¯Editionâ€¯â€“â€¯enforcesâ€¯asyncâ€¯fetchâ€¯andâ€¯PWAâ€¯serviceâ€¯workers.

###â€¯Phaseâ€¯Vâ€¯â€”â€¯Autonomousâ€¯Scalingâ€¯(âš™ï¸â€¯Future)

[ ]â€¯Multiâ€‘agentâ€¯schedulerâ€¯forâ€¯50+â€¯simultaneousâ€¯battles.
[ ]â€¯Distributedâ€¯storageâ€¯andâ€¯ELOâ€¯scoreboards.
[ ]â€¯Automaticâ€¯modelâ€¯deploymentâ€¯andâ€¯rollbackâ€¯basedâ€¯onâ€¯winâ€¯rateâ€¯thresholds.
[ ]â€¯Selfâ€‘evolvingâ€¯judgesâ€¯thatâ€¯retrainâ€¯criteriaâ€¯fromâ€¯humanâ€¯reviewâ€¯data.

###â€¯Phaseâ€¯VIâ€¯â€”â€¯Productionâ€¯/â€¯Researchâ€¯Launchâ€¯(ğŸŒâ€¯Future)

[ ]â€¯Publicâ€¯leaderboardâ€¯portal.
[ ]â€¯Paper:â€¯â€œAdversarialâ€¯Evolutionâ€¯ofâ€¯Autonomousâ€¯Codeâ€¯viaâ€¯Multiâ€‘LLMâ€¯Competition.â€
[ ]â€¯Integrationâ€¯withâ€¯GitHubâ€¯Actionsâ€¯â†’â€¯AIâ€¯botsâ€¯thatâ€¯autonomouslyâ€¯openâ€¯PRsâ€¯ofâ€¯winningâ€¯code.


ğŸ”¬â€¯Extendingâ€¯forâ€¯Otherâ€¯Domains
Toâ€¯createâ€¯newâ€¯Arenas:
1.â€¯Duplicateâ€¯thisâ€¯repo.
2.â€¯Replaceâ€¯language/frameworkâ€¯identifiersâ€¯inâ€¯prompts.py.
3.â€¯Designâ€¯aâ€¯newâ€¯judge.pyâ€¯withâ€¯yourâ€¯domainâ€™sâ€¯rules.
4.â€¯Optionallyâ€¯addâ€¯compiler/testâ€¯runnerâ€¯forâ€¯realâ€¯executionâ€¯scoring.
Example:â€¯Rustâ€¯Editionâ€¯â€”â€¯Asyncâ€¯+â€¯Tests
BANNED = [r"println!", r"thread::sleep", r"unwrap$$"]
REWARD  = [r"async fn", r"tokio::main", r"#[test]"]


ğŸâ€¯Vision

â€œAIâ€¯Codingâ€¯Arenaâ€¯isâ€¯theâ€¯evolutionaryâ€¯pressureâ€¯codeâ€¯hasâ€¯neverâ€¯had.â€

Eachâ€¯generationâ€¯learnsâ€¯fromâ€¯theâ€¯bloodâ€¯ofâ€¯itsâ€¯predecessor.
Judgesâ€¯enforceâ€¯truth;â€¯modelsâ€¯fightâ€¯forâ€¯perfection.
Theâ€¯endâ€¯gameâ€¯isâ€¯anâ€¯autonomousâ€¯softwareâ€¯ecosystemâ€¯whereâ€¯AIâ€¯agentsâ€¯continuouslyâ€¯generate,â€¯test,â€¯andâ€¯refineâ€¯theâ€¯worldâ€™sâ€¯codeâ€¯basesâ€¯â€”â€¯noâ€¯humanâ€¯babysittingâ€¯required.

###â€¯Runâ€¯yourâ€¯ownâ€¯Arena.â€¯Spawnâ€¯newâ€¯Judges.â€¯Letâ€¯theâ€¯LLMsâ€¯fight.
python controller.py

AIâ€¯Codingâ€¯Arena â€” Theâ€¯futureâ€¯ofâ€¯autonomousâ€¯softwareâ€¯evolution.
