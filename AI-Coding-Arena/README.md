ğŸ§ â€¯AIâ€¯Codingâ€¯Arenaâ€¯â€”â€¯Androidâ€¯SAFâ€¯Edition
A brutal, adversarial, selfâ€‘improving multiâ€‘agent coding platform.
Twoâ€¯LLMsâ€¯enter. Oneâ€¯survives.
They fight to produce the best Androidâ€¯14 Fileâ€¯Explorer using only the Storageâ€¯Accessâ€¯Frameworkâ€¯(SAF).
A mercilessâ€¯Judge ensures modernity, killing any code that touches legacy storage or fails to compile.
Only perfect, selfâ€‘evolving, standardsâ€‘compliant code climbs the leaderboard.

ğŸ§©â€¯Coreâ€¯Concept
AIâ€¯Codingâ€¯Arena is a multiâ€‘agent competitive softwareâ€‘generation platform.
Itâ€™s designed to test, evolve, and refine LLMâ€‘generated code using domainâ€‘specific judges and continuous feedback loops.
Each Round:
1.â€¯Twoâ€¯Agentsâ€¯(e.g.,â€¯Qwenâ€¯30Bâ€¯vsâ€¯CodeLlamaâ€¯13B)â€¯receiveâ€¯theâ€¯sameâ€¯challenge.
2.â€¯Eachâ€¯outputsâ€¯pureâ€¯codeâ€¯(HTML,â€¯Java,â€¯Rust,â€¯etc.),â€¯followingâ€¯formattingâ€¯rulesâ€¯enforcedâ€¯byâ€¯prompts.py.
3.â€¯Theâ€¯Judgeâ€¯(judge.py)â€¯immediatelyâ€¯scoresâ€¯eachâ€¯submission.
4.â€¯Controllerâ€¯(controller.py)â€¯logsâ€¯verdictsâ€¯andâ€¯automaticallyâ€¯feedsâ€¯theâ€¯judge'sâ€¯feedbackâ€¯intoâ€¯freshâ€¯promptsâ€¯forâ€¯theâ€¯nextâ€¯round.
5.â€¯Modelsâ€¯iterate,â€¯improve,â€¯andâ€¯competeâ€¯untilâ€¯onlyâ€¯stable,â€¯compilingâ€¯codeâ€¯remains.
Overâ€¯timeâ€¯theâ€¯Arenaâ€¯becomesâ€¯aâ€¯selfâ€‘containedâ€¯autonomousâ€¯codingâ€¯ecosystem:
LLMsâ€¯â†’â€¯produceâ€¯â†’â€¯testedâ€¯â†’â€¯scoredâ€¯â†’â€¯reâ€‘promptedâ€¯â†’â€¯refined.
*** added dueling version --- controller.py.dueling , use this if you want the LLMs to see and know what the other said. Results will vary with the prompt.
âš”ï¸â€¯Rulesâ€¯ofâ€¯Combatâ€¯(Androidâ€¯Edition)
Theâ€¯Judgeâ€¯executesâ€¯immediatelyâ€¯for:

â˜ ï¸â€¯Useâ€¯ofâ€¯java.io.File,â€¯getExternalStorageDirectory(),â€¯getParentFile(),â€¯Uri.fromFile(),â€¯orâ€¯anyâ€¯deprecatedâ€¯Fileâ€¯API.
ğŸ’¬â€¯Nonâ€‘codeâ€¯outputâ€¯(bulletâ€¯lists,â€¯releaseâ€¯notes,â€¯apologies).
ğŸ’¤â€¯Regression,â€¯hallucination,â€¯orâ€¯reintroductionâ€¯ofâ€¯legacyâ€¯logic.

Victoryâ€¯Conditions:

Fullâ€¯SAFâ€¯complianceâ€¯(ACTION_OPEN_DOCUMENT_TREE,â€¯takePersistableUriPermission,â€¯DocumentFile.fromTreeUri,â€¯DocumentsContract).
Modernâ€¯Androidâ€¯14â€¯SDKâ€¯compatibleâ€¯code.
Compilesâ€¯cleanlyâ€¯andâ€¯performsâ€¯allâ€¯CRUDâ€¯operationsâ€¯underâ€¯scopedâ€¯storage.


ğŸ—ï¸â€¯Projectâ€¯Structure



File
Purpose




prompts.py
Enforcesâ€¯â€œcodeâ€‘onlyâ€â€¯LLMâ€¯responses;â€¯injectsâ€¯pastâ€¯judgeâ€¯feedbackâ€¯intoâ€¯nextâ€¯promptâ€¯forâ€¯selfâ€‘improvement.


judge.py
Domainâ€‘specificâ€¯ruleâ€¯engine.â€¯Detectsâ€¯bannedâ€¯patterns,â€¯validatesâ€¯SAFâ€¯usage,â€¯andâ€¯scoresâ€¯submissionsâ€¯(0â€“10).


controller.py
Theâ€¯Arenaâ€¯engine.â€¯Orchestratesâ€¯rounds,â€¯launchesâ€¯models,â€¯logsâ€¯resultsâ€¯toâ€¯debates.db,â€¯andâ€¯reâ€‘feedsâ€¯verdictâ€¯backâ€¯toâ€¯agents.


main.pyâ€¯/â€¯API
FastAPIâ€¯endpointâ€¯forâ€¯submissionsâ€¯andâ€¯liveâ€¯resultsâ€¯streaming.


README.md
Thisâ€¯fileâ€¯â€”â€¯manifestoâ€¯+â€¯roadmap.

ğŸ§ â€¯Newâ€¯Autonomousâ€¯Infrastructureâ€¯(2025â€¯Update)

AIâ€¯Codingâ€¯Arena now runs fullyâ€¯selfâ€‘contained, with the following new subsystems:

- ğŸ§©â€¯**Autonomousâ€¯Continuationâ€¯Engine** â€”â€¯Each new battle automatically loads the previous transcript fromâ€¯`debates.db`, allowing projects to evolve over days or weeks without manual reset.

- ğŸ”‘â€¯**Tokenâ€¯Registrationâ€¯System** â€”â€¯`/api/register_topic`â€¯createsâ€¯shortâ€¯tokensâ€¯forâ€¯veryâ€¯largeâ€¯topicsâ€¯(>â€¯50â€¯kB)â€¯toâ€¯avoidâ€¯URLâ€¯lengthâ€¯errors duringâ€¯WebSocketâ€¯startup.  
  Battles now begin withâ€¯`ws://<host>/ws/debate?token=<token>&rounds=N`.

- ğŸ”„â€¯**Continuationâ€¯API** â€”â€¯`/api/continuation`â€¯exposesâ€¯theâ€¯mostâ€¯recentâ€¯judgeâ€¯verdictâ€¯andâ€¯codeâ€¯forâ€¯seamlessâ€¯sequelsâ€¯andâ€¯autonomousâ€¯reâ€‘prompts.

- âš–ï¸â€¯**Judgeâ€¯Interventionâ€¯Protocol** â€”â€¯Ifâ€¯aâ€¯modelâ€¯returnsâ€¯textâ€¯orâ€¯theâ€¯wrongâ€¯language,â€¯theâ€¯controllerâ€¯interjects,â€¯forcesâ€¯aâ€¯regeneration,â€¯andâ€¯reâ€‘feedsâ€¯aâ€¯correctionâ€¯messageâ€¯automatically.

- ğŸ†â€¯**Winningâ€¯Codeâ€¯Extraction** â€”â€¯Afterâ€¯verdict,â€¯theâ€¯Arenaâ€¯automaticallyâ€¯writesâ€¯theâ€¯lastâ€¯fencedâ€¯codeâ€¯blockâ€¯fromâ€¯theâ€¯winningâ€¯sideâ€¯toâ€¯`static/winning_code_<session_id>.txt`â€¯forâ€¯archivalâ€¯orâ€¯deployment.

- ğŸ’¾â€¯**Persistentâ€¯SQLiteâ€¯Archive** â€”â€¯Everyâ€¯sessionâ€¯(topicâ€¯+â€¯transcriptâ€¯+â€¯verdict)â€¯isâ€¯loggedâ€¯inâ€¯`debates.db`,â€¯supportingâ€¯continuation,â€¯leaderboards,â€¯andâ€¯crossâ€‘sessionâ€¯analysis.

- ğŸ’»â€¯**Browserâ€¯Webâ€¯Interfaceâ€¯v2** â€”â€¯Enhancedâ€¯`index.html`â€¯nowâ€¯handlesâ€¯tokenizedâ€¯topics,â€¯fileâ€‘sizeâ€¯safeâ€¯uploads,â€¯andâ€¯liveâ€¯judgeâ€¯streamingâ€¯throughâ€¯WebSocket.

These additions turn theâ€¯Arenaâ€¯fromâ€¯aâ€¯manualâ€¯competitionâ€¯sandboxâ€¯intoâ€¯aâ€¯trueâ€¯*autonomousâ€¯softwareâ€¯evolutionâ€¯environment*â€¯capableâ€¯ofâ€¯runningâ€¯endlessly


ğŸ”„â€¯Autonomousâ€¯Refâ€‘Feedâ€¯System
Coreâ€¯Mechanism:
1.â€¯Theâ€¯Judgeâ€¯returnsâ€¯verdictsâ€¯asâ€¯structuredâ€¯textâ€¯(score,â€¯reasoning,â€¯banned_hits,â€¯missing_patterns).
2.â€¯controller.pyâ€¯parsesâ€¯thisâ€¯verdictâ€¯intoâ€¯aâ€¯conciseâ€¯summary.
3.â€¯Thatâ€¯summaryâ€¯isâ€¯reâ€‘appendedâ€¯toâ€¯eachâ€¯modelâ€™sâ€¯promptâ€¯forâ€¯theâ€¯nextâ€¯round,â€¯e.g.:
Previousâ€¯Verdictâ€¯Summary:
â€¢â€¯Failedâ€¯SAFâ€¯validationâ€¯â€”â€¯missingâ€¯takePersistableUriPermission
â€¢â€¯Usedâ€¯grantUriPermissionâ€¯instead
â€¢â€¯Requiredâ€¯patterns:â€¯ACTION_OPEN_DOCUMENT_TREE,â€¯takePersistableUriPermission,â€¯DocumentFile.fromTreeUri

â†’â€¯Fixâ€¯allâ€¯issuesâ€¯andâ€¯resubmitâ€¯entireâ€¯MainActivity.javaâ€¯asâ€¯Androidâ€¯14â€¯codeâ€‘only.

Eachâ€¯roundâ€¯thereforeâ€¯includesâ€¯semanticallyâ€¯relevantâ€¯feedbackâ€¯fromâ€¯theâ€¯Judge,â€¯drivingâ€¯modelâ€¯optimizationâ€¯withoutâ€¯humanâ€¯input.

âš–ï¸â€¯Judgeâ€¯Evolutionâ€¯andâ€¯Metaâ€‘Learningâ€¯(ğŸ§ â€¯New)
Theâ€¯Judgeâ€¯isâ€¯noâ€¯longerâ€¯staticâ€¯â€”â€¯itâ€¯evolvesâ€¯too.
Yourâ€¯Arenaâ€™sâ€¯nextâ€¯frontierâ€¯isâ€¯coâ€‘evolution:â€¯notâ€¯justâ€¯modelsâ€¯learningâ€¯toâ€¯codeâ€¯better,â€¯butâ€¯judgesâ€¯learningâ€¯toâ€¯judgeâ€¯better.
Currentâ€¯Stage:â€¯Regexâ€¯patternâ€¯matchingâ€¯andâ€¯ruleâ€‘basedâ€¯scoring.
Nextâ€¯Stages:
1.â€¯Semanticâ€¯(Aâ€¯Sâ€¯Tâ€¯Parsing)â€¯â€“â€¯Integrateâ€¯JavaParserâ€¯orâ€¯KSPâ€¯forâ€¯trueâ€¯syntacticâ€¯validation.
2.â€¯Compilationâ€¯Gradingâ€¯â€“â€¯Dockerizedâ€¯Gradleâ€¯buildsâ€¯confirmâ€¯codeâ€¯compilesâ€¯andâ€¯producesâ€¯workingâ€¯APKs.
3.â€¯Runtimeâ€¯Simulationâ€¯â€“â€¯Sandboxâ€¯eachâ€¯APKâ€¯inâ€¯anâ€¯emulatorâ€¯(AVDâ€¯/â€¯Robolectric)â€¯toâ€¯executeâ€¯fileâ€¯operationsâ€¯andâ€¯captureâ€¯logsâ€¯forâ€¯scoring.
4.â€¯Weightedâ€¯Scoringâ€¯â€“â€¯Combineâ€¯staticâ€¯checksâ€¯(40â€¯%),â€¯compileâ€¯statusâ€¯(40â€¯%),â€¯andâ€¯behaviorâ€¯testsâ€¯(20â€¯%).
5.â€¯Metaâ€‘Feedbackâ€¯Loopâ€¯â€“â€¯Storeâ€¯casesâ€¯whereâ€¯Judgeâ€¯verdictsâ€¯misalignâ€¯withâ€¯realâ€¯runtimeâ€¯testsâ€¯â†’â€¯automaticallyâ€¯updateâ€¯scoringâ€¯rulesâ€¯=â€¯selfâ€‘correctingâ€¯Judge.
In futureâ€¯phases,â€¯theâ€¯Judgeâ€¯itselfâ€¯becomesâ€¯anâ€¯LLMâ€¯QAâ€¯agentâ€¯trainedâ€¯onâ€¯itsâ€¯ownâ€¯mistakes.
The arenaâ€¯thenâ€¯hostsâ€¯twoâ€¯speciesâ€¯ofâ€¯AIâ€¯coâ€‘evolving:â€¯codersâ€¯andâ€¯critics.

ğŸš€â€¯Runningâ€¯theâ€¯Arena
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000

Accessâ€¯theâ€¯Arenaâ€¯at:â€¯http://localhost:8000
###â€¯APIâ€¯Endpoints
-â€¯POSTâ€¯/submitâ€¯â€“â€¯submitâ€¯aâ€¯pairâ€¯ofâ€¯codesâ€¯(Aâ€¯andâ€¯B).
-â€¯GETâ€¯/verdictsâ€¯â€“â€¯streamâ€¯latestâ€¯results.
-â€¯GETâ€¯/leaderboardâ€¯â€“â€¯ELOâ€‘styleâ€¯rankingâ€¯ofâ€¯modelsâ€¯byâ€¯winâ€¯rateâ€¯andâ€¯averageâ€¯score.
ğŸ§¾â€¯README.md Additions
Add this new section below your setup instructions so anyone cloning later can run it without confusion:

ğŸ’¬â€¯Web Interfaceâ€¯â€“â€¯Updated Tokenâ€¯System
Recent versions ofâ€¯AIâ€¯Debateâ€¯Arena use aâ€¯tokenâ€‘based topic transfer mechanism to handle large debate histories safely:


Continuations:
Each new debate automatically fetches the previous transcript via
GETâ€¯/api/continuation?limit=1&round_no=<n>.


Token registration:
The browser thenâ€¯posts the long prompt body to
POSTâ€¯/api/register_topicâ€¯â†’â€¯receives a shortâ€¯token, e.g.â€¯02353ffc732f56f5.


WebSocket startup:
The debate begins with
ws://<host>/ws/debate?token=<token>&rounds=...,
eliminating URLâ€¯length limitations and preventingâ€¯400â€¯errors even withâ€¯veryâ€¯longâ€¯histories.


Server preload:
Topics are stored temporarily inâ€¯memoryâ€¯(TOPIC_CACHE)â€¯and removed after retrieval to keep memory use low.


Typical workflow
python3 -m uvicorn main:app --reload --host 0.0.0.0 --port 8000

Then open http://localhost:8000/static/index.html
â†’â€¯press STARTâ€¯DEBATEâ€¯to begin.


Expected log output
TOPIC length: 79004
Roundâ€¯13â€¯â€”â€¯Continuation ofâ€¯Roundâ€¯12
...

This confirms the previous transcript was loaded successfully.
ğŸ—ºï¸â€¯Fullâ€¯Roadmap
###â€¯Phaseâ€¯Iâ€¯â€”â€¯Foundationâ€¯(âœ…â€¯Complete)
âœ…â€¯Androidâ€¯14â€¯Judgeâ€¯eliminatesâ€¯legacyâ€¯Fileâ€¯APIs.
âœ…â€¯SAFâ€¯structuralâ€¯validationâ€¯withâ€¯regexâ€¯andâ€¯scoring.
âœ…â€¯Arenaâ€¯Controllerâ€¯handlesâ€¯A/Bâ€¯generationâ€¯cyclesâ€¯andâ€¯storesâ€¯debatesâ€¯inâ€¯SQLite.
###â€¯Phaseâ€¯IIâ€¯â€”â€¯Autonomousâ€¯Feedbackâ€¯Loopâ€¯(ğŸš€â€¯Active)
[x]â€¯Verdictâ€¯parserâ€¯extractsâ€¯structuredâ€¯feedback.
[x]â€¯Reâ€‘feedâ€¯systemâ€¯autoâ€‘insertsâ€¯Judgeâ€¯assessmentâ€¯intoâ€¯nextâ€¯modelâ€¯prompt.
[x]â€¯Runâ€¯continuousâ€¯evolutionâ€¯roundsâ€¯untilâ€¯stability.
[x]â€¯Tokenâ€¯registrationâ€¯andâ€¯continuationâ€¯APIsâ€¯enableâ€¯autonomousâ€¯multiâ€‘sessionâ€¯runs.  
[x]â€¯Judgeâ€¯interventionâ€¯handlesâ€¯invalidâ€¯modelâ€¯outputsâ€¯withoutâ€¯humanâ€¯assistance.  
[x]â€¯Winningâ€¯codeâ€¯extractionâ€¯createsâ€¯deployableâ€¯artifactsâ€¯forâ€¯eachâ€¯session.  
[x]â€¯Autonomousâ€¯continuationâ€¯engineâ€¯linksâ€¯debatesâ€¯overâ€¯timeâ€¯â†’â€¯selfâ€‘evolvingâ€¯projects.

###â€¯Phaseâ€¯IIIâ€¯â€”â€¯Beyondâ€¯Regexâ€¯(ğŸ§ â€¯Planned)
[ ]â€¯Integrateâ€¯ASTâ€‘basedâ€¯analyzersâ€¯(JavaParserâ€¯/â€¯Kotlinâ€¯KSP).
[ ]â€¯Dockerizedâ€¯Gradleâ€¯buildâ€¯toâ€¯confirmâ€¯actualâ€¯compilationâ€¯&â€¯APKâ€¯build.
[ ]â€¯Runtimeâ€¯sandboxâ€¯toâ€¯executeâ€¯instrumentedâ€¯testsâ€¯onâ€¯emulatorâ€¯containersâ€¯(Googleâ€¯AVDâ€¯+â€¯CIâ€¯pipeline).
[ ]â€¯Addâ€¯Judgeâ€¯Metaâ€‘Learningâ€¯Pipelineâ€¯forâ€¯selfâ€‘trainingâ€¯onâ€¯verdictâ€¯mismatches.
###â€¯Phaseâ€¯IVâ€¯â€”â€¯Crossâ€‘Languageâ€¯Arenasâ€¯(ğŸ”¥â€¯Next)
[ ]â€¯Rustâ€¯Editionâ€¯â€“â€¯Judgeâ€¯rewardsâ€¯async/await,â€¯unitâ€¯tests,â€¯forbidsâ€¯println!()â€¯debugging.
[ ]â€¯Pythonâ€¯Editionâ€¯â€“â€¯forbidsâ€¯print()â€¯logs,â€¯rewardsâ€¯asyncioâ€¯andâ€¯pytestâ€¯coverage.
[ ]â€¯C++â€¯Editionâ€¯â€“â€¯penalizesâ€¯manualâ€¯new/delete,â€¯rewardsâ€¯RAIIâ€¯andâ€¯constexpr.
[ ]â€¯Webâ€¯Editionâ€¯â€“â€¯enforcesâ€¯fetchâ€¯withâ€¯async/awaitâ€¯andâ€¯PWAâ€¯serviceâ€¯workers.
###â€¯Phaseâ€¯Vâ€¯â€”â€¯Autonomousâ€¯Scalingâ€¯(âš™ï¸â€¯Future)
[ ]â€¯Multiâ€‘agentâ€¯schedulerâ€¯forâ€¯50+â€¯simultaneousâ€¯battles.
[ ]â€¯Distributedâ€¯storageâ€¯andâ€¯ELOâ€¯scoreboards.
[ ]â€¯Automaticâ€¯modelâ€¯deploymentâ€¯andâ€¯rollbackâ€¯basedâ€¯onâ€¯winâ€¯rateâ€¯thresholds.
[ ]â€¯Evolvingâ€¯Judgesâ€¯thatâ€¯retrainâ€¯criteriaâ€¯fromâ€¯humanâ€¯reviewâ€¯orâ€¯runtimeâ€¯telemetry.
###â€¯Phaseâ€¯VIâ€¯â€”â€¯Productionâ€¯/â€¯Researchâ€¯Launchâ€¯(ğŸŒâ€¯Future)
[ ]â€¯Publicâ€¯leaderboardâ€¯portal.
[ ]â€¯Publishâ€¯whiteâ€¯paper:â€¯â€œAdversarialâ€¯Coâ€‘evolutionâ€¯ofâ€¯Codeâ€¯andâ€¯Evaluationâ€¯inâ€¯Multiâ€‘LLMâ€¯Systems.â€
[ ]â€¯Integrationâ€¯withâ€¯GitHubâ€¯Actionsâ€¯â†’â€¯AIâ€¯botsâ€¯thatâ€¯autonomouslyâ€¯openâ€¯PRsâ€¯ofâ€¯winningâ€¯code.

ğŸ”¬â€¯Extendingâ€¯forâ€¯Otherâ€¯Domains
Toâ€¯createâ€¯newâ€¯Arenas:
1.â€¯Duplicateâ€¯thisâ€¯repo.
2.â€¯Replaceâ€¯language/frameworkâ€¯identifiersâ€¯inâ€¯prompts.py.
3.â€¯Designâ€¯aâ€¯newâ€¯judge.pyâ€¯withâ€¯yourâ€¯domainâ€™sâ€¯rules.
4.â€¯Optionallyâ€¯addâ€¯compiler/testâ€¯runnerâ€¯forâ€¯realâ€¯executionâ€¯scoring.
Exampleâ€¯â€“â€¯Rustâ€¯Edition:
BANNED = [r"println!", r"thread::sleep", r"unwrap$$"]
REWARD = [r"async fn", r"tokio::main", r"#[test]"]


ğŸâ€¯Vision

â€œAIâ€¯Codingâ€¯Arenaâ€¯isâ€¯theâ€¯evolutionaryâ€¯pressureâ€¯codeâ€¯hasâ€¯neverâ€¯had.â€

Eachâ€¯generationâ€¯learnsâ€¯fromâ€¯theâ€¯bloodâ€¯ofâ€¯itsâ€¯predecessor.
Judgesâ€¯enforceâ€¯truth;â€¯modelsâ€¯fightâ€¯forâ€¯perfection.
Theâ€¯endâ€¯gameâ€¯isâ€¯anâ€¯autonomousâ€¯softwareâ€¯ecosystemâ€¯whereâ€¯AIâ€¯agentsâ€¯continuouslyâ€¯generate,â€¯test,â€¯andâ€¯refineâ€¯theâ€¯worldâ€™sâ€¯codeâ€¯basesâ€¯â€”â€¯noâ€¯humanâ€¯babysittingâ€¯required.

###â€¯Runâ€¯yourâ€¯ownâ€¯Arena.â€¯Spawnâ€¯newâ€¯Judges.â€¯Letâ€¯theâ€¯LLMsâ€¯fight.
python controller.py

AIâ€¯Codingâ€¯Arenaâ€¯â€”â€¯Theâ€¯futureâ€¯ofâ€¯autonomousâ€¯softwareâ€¯evolution
ğŸš€â€¯Howâ€¯toâ€¯Useâ€¯AIâ€¯Codingâ€¯Arenaâ€¯toâ€¯Buildâ€¯Code
AIâ€¯Codingâ€¯Arena lets two codeâ€‘generation models debate a project idea while aâ€¯third model acts as aâ€¯judge.  Each run creates or improves code, and the judge decides which side produced the stronger implementation.
1ï¸âƒ£â€¯Start the server
cd ~/Downloads/ai-debate-arena-main/AI-Coding-Arena
python3 -m uvicorn main:app --reload --host 0.0.0.0 --port 8000

Then open your browser at
http://localhost:8000/static/index.html

2ï¸âƒ£â€¯Choose your models
At the top of the web page:

Sideâ€¯Aâ€¯â€“â€¯main implementer (e.g.â€¯qwen2â€‘coder:14bâ€‘instruct)
Sideâ€¯Bâ€¯â€“â€¯reviewer/improver (e.g.â€¯mistral:8x7bâ€‘instruct)
Judgeâ€¯â€“â€¯evaluator (e.g.â€¯qwen3â€‘coder:30b)

You can change them every runâ€”each debate is stored separately.

3ï¸âƒ£â€¯Enter aâ€¯topic (the task)
Paste your prompt into theâ€¯Debateâ€¯Topicâ€¯box, e.g.:
Before implementation, summarise what you understood from the previous transcript.
If you do not see any PREVIOUSâ€¯TRANSCRIPT section, output "Iâ€¯doâ€¯notâ€¯seeâ€¯it" and stop.

Implement a complete Androidâ€¯Storageâ€¯Accessâ€¯Frameworkâ€¯(SAF) app:
â€“â€¯Launch folder picker with ActivityResultLauncher + ACTION_OPEN_DOCUMENT_TREE
â€“â€¯Persist permissions with takePersistableUriPermission()
â€“â€¯Perform CRUD operations using DocumentFile
â€“â€¯Include a UI (Compose or XML)
The Judge penalises any nonâ€‘Android languages or missing items.


4ï¸âƒ£â€¯Run and watch the debate
Press STARTâ€¯DEBATE.
Youâ€™ll see streaming output:
==================== ROUND 1 | SIDE A ====================
Valid code extracted (182 lines). Project evolving...

...
JUDGE INVOKED â€” FINAL VERDICT INCOMING...
SAFâ€¯COMPLETE â€”â€¯foundâ€¯4/4â€¯criticalâ€¯patterns
Winner:â€¯Sideâ€¯B


â€œValidâ€¯codeâ€¯extractedâ€¦â€â€¯â†’â€¯the app detected fenced code and accepted it.
â€œJUDGEâ€¯INTERVENTIONâ€¦â€â€¯â†’â€¯formatting problems; the model was asked to retry.
The final verdict tells you which sideâ€™s code â€œwon.â€


5ï¸âƒ£â€¯Continue improving
After every run the database debates.db saves the whole transcript and verdict.
To extend an existing project just start another debateâ€”the system automatically loads the last session as context.

6ï¸âƒ£â€¯Find your results

Full debates: stored inâ€¯debates.db
Judged winning code (optional):
static/winning_code_<session_id>.txt
Inspect recent runs:
sqlite3 debates.db "SELECT id, timestamp, topic FROM debates ORDER BY id DESC LIMIT 5;"



7ï¸âƒ£â€¯Typical workflow

Brainstorm or import a task description.
Run a short debate (3â€“6â€¯rounds).
Review the judged code.
Launch another debate to refine the same project or switch models to compare results.


ğŸ’¡â€¯Tips



Useâ€‘case
Recommendation
Make it yours. Ask an Ai to help build the inital prompt for you task. Get the AI to help you modify judge.py to assess the code.
Use your imagination, there is no limit.



Starting fresh idea
Write a descriptive topic that defines deliverables.


Continuing existing code
Simply press STARTâ€¯DEBATE; continuation loads automatically.


Comparing models
Change Sideâ€¯Aâ€¯orâ€¯B; keep the same task.


Backup data
Copy debates.db â€” it contains every session transcript and verdict.




